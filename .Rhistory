amal <- read.csv("C:/Users/danek/Desktop/jkmnet/data/inputs/data_all_daily.csv")
amalid <- amal[amal$ID == sensorId,c("date","moisture")]
amalid$date <- as.Date(amalid$date)
rownames(amalid) = 1:length(amalid$moisture)
unique(amal$ID)
plot(amal[amal$ID == 93148339,]$moisture, type = "l")
View(vsl)
vsl[vsl$sensor_id == 93148339,]
# ---------------------------
# IMPORT LIBRARIES
# ---------------------------
library(data.table)
library(foreach)
library(doParallel)
# ---------------------------
# CREATE DEFAULT TEMPLATE
# ---------------------------
make_defaults <- function() {
list(
data = list(
data_file = "../../data/data_all_daily.csv",
id        = "94206029",
id_col    = "ID",
columns   = c("T1","T2","T3","moisture"),
timestamp = "date"
),
model = list(
trainer       = "batch",
architecture  = c(8, 6, 2),
input_numbers = c(0, 0, 0, 1),
activation    = "RELU",
weight_init   = "RANDOM"
),
training = list(
ensemble_runs = 20,
max_iterations = 500L,
max_metrics_step = 10,
max_error      = 0.002,
learning_rate  = 0.001,
shuffle        = TRUE,
seed           = 0,
batch_size     = 30L,
train_fraction = 0.8,
split_shuffle  = FALSE,
transform      = "MINMAX",
transform_alpha = 0.1,
exclude_last_col_from_transform = FALSE,
remove_na_before_calib          = FALSE
),
paths = list(
out_dir = "outputs/",
log_dir = "outputs/logs/",
calib_mat = "outputs/calib_mat.csv",
weights_csv_init = "outputs/weights/weights_init.csv",
weights_bin_init = "outputs/weights/weights_init.bin",
weights_vec_csv_init = "outputs/weights/weights_init_vector.csv",
weights_vec_bin_init = "outputs/weights/weights_init_vector.bin",
weights_csv = "outputs/weights/weights_final.csv",
weights_bin = "outputs/weights/weights_final.bin",
weights_vec_csv = "outputs/weights/weights_final_vector.csv",
weights_vec_bin = "outputs/weights/weights_final_vector.bin",
real_calib = "outputs/calib_real.csv",
pred_calib = "outputs/calib_pred.csv",
real_valid = "outputs/valid_real.csv",
pred_valid = "outputs/valid_pred.csv",
metrics_cal = "outputs/calib_metrics.csv",
metrics_val = "outputs/valid_metrics.csv",
run_info = "outputs/run_info.csv",
errors_csv  = "outputs/errors.csv",
pattern_indices = "outputs/pattern_indices.csv"
)
)
}
# ---------------------------
# RENDER TEXT FROM THE FILE
# ---------------------------
render_ini_text <- function(cfg) {
fmt_bool <- function(x) tolower(as.character(as.logical(x)))
fmt_vec  <- function(x) paste(x, collapse = ", ")
fmt_num  <- function(x) if (is.numeric(x)) sprintf("%g", x) else sub(",", ".", as.character(x), fixed = TRUE)
lines <- c(
"; config_model.ini",
"",
"; [data]",
paste0("data_file = ", cfg$data$data_file, "  ; hourly/daily data for the run"),
paste0("id = ", cfg$data$id),
paste0("id_col = ", cfg$data$id_col),
paste0("columns = ", fmt_vec(cfg$data$columns)),
paste0("timestamp = ", cfg$data$timestamp, "  ; date (daily data), hour_start (hourly data)"),
"",
"; [model]",
paste0("trainer = ", cfg$model$trainer, " ; online, batch, online_epoch, batch_epoch"),
paste0("architecture = ", fmt_vec(cfg$model$architecture)),
paste0("input_numbers = ", fmt_vec(cfg$model$input_numbers), "  ; length must match number of columns (T1,T2,T3,moisture)"),
paste0("activation = ", cfg$model$activation, "  ; RELU, SIGMOID, LINEAR, TANH, GAUSSIAN, ..."),
paste0("weight_init = ", cfg$model$weight_init, "  ; RANDOM, LHS, LHS2"),
"",
"; [training]",
paste0("ensemble_runs = ", cfg$training$ensemble_runs),
paste0("max_iterations = ", cfg$training$max_iterations),
paste0("max_metrics_step = ", cfg$training$max_metrics_step, "  ; total number of metrics written into file = ceil(max_iterations % max_metrics_step)"),
paste0("max_error = ", fmt_num(cfg$training$max_error)),
paste0("learning_rate = ", fmt_num(cfg$training$learning_rate)),
paste0("shuffle = ", fmt_bool(cfg$training$shuffle)),
paste0("seed = ", cfg$training$seed, "  ; 0 = random"),
paste0("batch_size = ", cfg$training$batch_size, "  ; used only for batch trainer"),
paste0("train_fraction = ", fmt_num(cfg$training$train_fraction)),
paste0("split_shuffle = ", fmt_bool(cfg$training$split_shuffle), "  ; false = chronological, true = random train/validation split"),
paste0("transform = ", cfg$training$transform, "     ; NONE, MINMAX, NONLINEAR, ZSCORE"),
paste0("transform_alpha = ", fmt_num(cfg$training$transform_alpha), "  ; param"),
paste0("exclude_last_col_from_transform = ", fmt_bool(cfg$training$exclude_last_col_from_transform)),
paste0("remove_na_before_calib = ", fmt_bool(cfg$training$remove_na_before_calib)),
"",
"; [optimization]",
"pso_optimize = false",
"",
"; [paths]",
paste0("out_dir = ", cfg$paths$out_dir),
paste0("log_dir = ", cfg$paths$log_dir),
paste0("calib_mat = ", cfg$paths$calib_mat),
paste0("weights_csv_init = ", cfg$paths$weights_csv_init),
paste0("weights_bin_init = ", cfg$paths$weights_bin_init),
paste0("weights_vec_csv_init = ", cfg$paths$weights_vec_csv_init),
paste0("weights_vec_bin_init = ", cfg$paths$weights_vec_bin_init),
paste0("weights_csv = ", cfg$paths$weights_csv),
paste0("weights_bin = ", cfg$paths$weights_bin),
paste0("weights_vec_csv = ", cfg$paths$weights_vec_csv),
paste0("weights_vec_bin = ", cfg$paths$weights_vec_bin),
paste0("real_calib = ", cfg$paths$real_calib),
paste0("pred_calib = ", cfg$paths$pred_calib),
paste0("real_valid = ", cfg$paths$real_valid),
paste0("pred_valid = ", cfg$paths$pred_valid),
paste0("metrics_cal = ", cfg$paths$metrics_cal),
paste0("metrics_val = ", cfg$paths$metrics_val),
paste0("run_info = ", cfg$paths$run_info),
paste0("errors_csv = ", cfg$paths$errors_csv),
paste0("pattern_indices = ", cfg$paths$pattern_indices)
)
paste(lines, collapse = "\n")
}
normalize_dir <- function(p) sub("[/\\\\]+$", "", p)
ensure_dir <- function(p) if (!dir.exists(p)) dir.create(p, recursive = TRUE)
# ---------------------------
# PATH TO DATA FILE
# ---------------------------
#data_path <- "data_all_daily.csv"  # MetaVO
data_path <- "data/inputs/data_all_daily.csv"  # MJ local
data_file <- fread(data_path)
setwd("C:/Users/danek/Desktop/jkmnet")
normalize_dir <- function(p) sub("[/\\\\]+$", "", p)
ensure_dir <- function(p) if (!dir.exists(p)) dir.create(p, recursive = TRUE)
# ---------------------------
# PATH TO DATA FILE
# ---------------------------
#data_path <- "data_all_daily.csv"  # MetaVO
data_path <- "data/inputs/data_all_daily.csv"  # MJ local
data_file <- fread(data_path)
setDT(data_file)
# select IDs - only IDs with more then 100 data points
# minimum number of records required
min_n <- 100
max_ids <- 2 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
param_grid <- CJ(
id = ids,
trainer = c("batch_epoch"),
activation = c("RELU"),
inputs = list(
c(0, 0, 0, 2),
c(0, 0, 0, 14)),
transform = c("NONLINEAR", "ZSCORE"),
alpha = c(0.9),
architecture = list(
# c(5, 1),
# c(5, 2),
c(5, 3),
# c(15, 1),
# c(15, 2),
#c(15, 3),
# c(5, 5, 1),
# c(5, 5, 2),
#c(5, 5, 3),
# c(10, 10, 1),
# c(10, 10, 2),
# c(10, 10, 3),
# c(5, 15, 1),
# c(5, 15, 2),
#c(5, 15, 3),
# c(5, 10, 10, 1),
# c(5, 10, 10, 2),
#c(5, 10, 10, 3)
), sorted = FALSE
)
param_grid <- CJ(
id = ids,
trainer = c("batch_epoch"),
activation = c("RELU"),
inputs = list(
c(0, 0, 0, 2),
c(0, 0, 0, 14)),
transform = c("NONLINEAR", "ZSCORE"),
alpha = c(0.9),
architecture = list(
# c(5, 1),
# c(5, 2),
c(5, 3),
# c(15, 1),
# c(15, 2),
#c(15, 3),
# c(5, 5, 1),
# c(5, 5, 2),
#c(5, 5, 3),
# c(10, 10, 1),
# c(10, 10, 2),
# c(10, 10, 3),
# c(5, 15, 1),
# c(5, 15, 2),
#c(5, 15, 3),
# c(5, 10, 10, 1),
# c(5, 10, 10, 2),
c(5, 10, 10, 3)
), sorted = FALSE
)
# add epochs based on architecture depth
param_grid[, epochs := ifelse(lengths(architecture) < 3, 500, 1000)]
# add n of outputs based on architecture
param_grid[, outputs := as.integer(sapply(architecture, function(x) tail(unlist(x), 1L)))]
# function to create custom case name from parameters
make_case_name <- function(...) {
args <- list(...)
# filtr prázdných hodnot (beze změny)
args <- args[!vapply(args, function(x) {
if (is.null(x)) return(TRUE)
if (all(is.na(x))) return(TRUE)
if (length(x) == 0L) return(TRUE)
if (identical(x, "")) return(TRUE)
FALSE
}, logical(1))]
# rozbal listy
if (!is.null(args$architecture)) args$architecture <- unlist(args$architecture)
if (!is.null(args$inputs))       args$inputs       <- unlist(args$inputs)
# formát vektorů
if (!is.null(args$architecture)) args$architecture <- paste(args$architecture, collapse = "x")
if (!is.null(args$inputs))       args$inputs       <- paste(args$inputs,       collapse = "x")
# --- změna č. 1: alpha bez tečky (0.015 -> 0p015) ---
if (!is.null(args$alpha) || !is.null(args$transform_alpha)) {
a <- if (!is.null(args$alpha)) args$alpha else args$transform_alpha
atxt <- format(round(as.numeric(a), 6), trim = TRUE, scientific = FALSE)
atxt <- sub("\\.?0+$", "", atxt)     # uřízni zbytečné nuly
atxt <- gsub("\\.", "p", atxt)       # tečka -> p
args$a <- paste0("a", atxt)
args$alpha <- NULL
args$transform_alpha <- NULL
}
# iterations alias (beze změny)
if (!is.null(args$iterace) || !is.null(args$max_iterations)) {
e <- if (!is.null(args$iterace)) args$iterace else args$max_iterations
args$e <- paste0("e", e)
args$iterace <- NULL
args$max_iterations <- NULL
}
# sestavení částí (upravené prefixy in/out, arch do názvu nedáváme)
parts <- vapply(seq_along(args), function(i) {
k <- names(args)[i]; v <- args[[i]]
if (k %in% c("id","trainer","activation","transform")) {
as.character(v)
} else if (k == "inputs") {
paste0("in", v)
} else if (k == "outputs") {
paste0("out", v)
} else if (k == "architecture") {
paste0("arch", v)
} else if (k %in% c("a","e")) {
v   # už připravené tokeny
} else {
paste0(k, v)
}
}, character(1))
parts <- parts[nzchar(parts)]
case_name <- paste(parts, collapse = "_")
# --- změna č. 2: globální sanity – žádné tečky ve jménu složky ---
case_name <- gsub("[^A-Za-z0-9._-]", "", case_name)
case_name <- gsub("\\.", "p", case_name)  # tečky pryč i kdyby se nějaká procpala
case_name <- gsub("_+", "_", case_name)
if (nchar(case_name) > 200) case_name <- substr(case_name, 1, 200)
case_name
}
# ---------------------------
# GENERATE CONGIFURATION FILE
# ---------------------------
generate_config_case <- function(
root_dir,
id,
epoch,
data_source,
#bin_path = "/storage/projects-du-praha/czu-fzp-hcv/software/JKMNet/bin/JKMNet",  # MetaVO
bin_path = "bin/JKMNet",  # MJ local
copy_data_into_inputs = FALSE,
overwrite_inputs = TRUE,
filter_input_by_id = TRUE,
select_only_model_columns = TRUE,
inputs = NULL,
outputs = NULL,
# optional overrides
trainer = NULL, architecture = NULL, input_numbers = NULL,
activation = NULL, weight_init = NULL,
ensemble_runs = NULL,
learning_rate = NULL, seed = NULL, split_shuffle = NULL,
columns = NULL, timestamp = NULL, data_file = NULL,
max_error = NULL, shuffle = NULL, batch_size = NULL,
train_fraction = NULL, transform = NULL, transform_alpha = NULL,
exclude_last_col_from_transform = NULL, remove_na_before_calib = NULL,
max_iterations = NULL, max_metrics_step = NULL
) {
cfg <- make_defaults()
# basic updates
cfg$data$id <- as.character(id)
cfg$training$max_iterations <- as.integer(epoch)
# apply overrides if given
if (!is.null(columns))       cfg$data$columns <- columns
if (!is.null(timestamp))     cfg$data$timestamp <- timestamp
if (!is.null(data_file))     cfg$data$data_file <- data_file
if (!is.null(trainer))       cfg$model$trainer <- trainer
if (!is.null(architecture)) {
cfg$model$architecture <- as.numeric(unlist(architecture))
}
#if (!is.null(input_numbers)) cfg$model$input_numbers <- input_numbers
if (!is.null(input_numbers)) {
cfg$model$input_numbers <- as.numeric(unlist(input_numbers))
} else if (!is.null(inputs)) {
cfg$model$input_numbers <- as.numeric(unlist(inputs))
}
if (!is.null(activation))    cfg$model$activation <- activation
if (!is.null(weight_init))   cfg$model$weight_init <- weight_init
if (!is.null(ensemble_runs)) cfg$training$ensemble_runs <- as.integer(ensemble_runs)
if (!is.null(learning_rate)) cfg$training$learning_rate <- as.numeric(learning_rate)
if (!is.null(seed))          cfg$training$seed <- as.integer(seed)
if (!is.null(split_shuffle)) cfg$training$split_shuffle <- isTRUE(split_shuffle)
if (!is.null(max_iterations))cfg$training$max_iterations <- as.integer(max_iterations)
if (!is.null(max_metrics_step)) cfg$training$max_metrics_step <- as.integer(max_metrics_step)
if (!is.null(max_error))     cfg$training$max_error <- as.numeric(max_error)
if (!is.null(shuffle))       cfg$training$shuffle <- isTRUE(shuffle)
if (!is.null(batch_size))    cfg$training$batch_size <- as.integer(batch_size)
if (!is.null(train_fraction))cfg$training$train_fraction <- as.numeric(train_fraction)
if (!is.null(transform))     cfg$training$transform <- transform
if (!is.null(transform_alpha)) cfg$training$transform_alpha <- as.numeric(transform_alpha)
if (!is.null(exclude_last_col_from_transform))
cfg$training$exclude_last_col_from_transform <- isTRUE(exclude_last_col_from_transform)
if (!is.null(remove_na_before_calib))
cfg$training$remove_na_before_calib <- isTRUE(remove_na_before_calib)
# create case name
case_name <- make_case_name(
id = cfg$data$id,
trainer = cfg$model$trainer,
activation = cfg$model$activation,
inputs = if (!is.null(inputs)) inputs else length(cfg$data$columns),
outputs = if (!is.null(outputs)) outputs else tail(cfg$model$architecture, 1L),
transform = cfg$training$transform,
alpha = cfg$training$transform_alpha,
architecture = cfg$model$architecture,
max_iterations = cfg$training$max_iterations
)
case_dir <- file.path(normalize_dir(root_dir), case_name)
ensure_dir(case_dir)
settings_dir <- file.path(case_dir, "settings"); ensure_dir(settings_dir)
outputs_dir  <- file.path(case_dir, "outputs");  ensure_dir(outputs_dir)
weights_dir <- file.path(outputs_dir, "weights"); ensure_dir(weights_dir)
metrics_dir <- file.path(outputs_dir, "metrics"); ensure_dir(metrics_dir)
logs_dir <- file.path(outputs_dir, "logs"); ensure_dir(logs_dir)
# rewrite outputs paths
cfg$paths$out_dir <- "outputs"
cfg$paths$calib_mat <- file.path("outputs", "calib_mat.csv")
cfg$paths$weights_csv_init <- file.path("outputs", "weights", "weights_init.csv")
cfg$paths$weights_bin_init <- file.path("outputs", "weights", "weights_init.bin")
cfg$paths$weights_vec_csv_init <- file.path("outputs", "weights", "weights_init_vector.csv")
cfg$paths$weights_vec_bin_init <- file.path("outputs", "weights", "weights_init_vector.bin")
cfg$paths$weights_csv <- file.path("outputs", "weights", "weights_final.csv")
cfg$paths$weights_bin <- file.path("outputs", "weights", "weights_final.bin")
cfg$paths$weights_vec_csv <- file.path("outputs", "weights", "weights_final_vector.csv")
cfg$paths$weights_vec_bin <- file.path("outputs", "weights", "weights_final_vector.bin")
cfg$paths$real_calib <- file.path("outputs", "calib_real.csv")
cfg$paths$pred_calib <- file.path("outputs", "calib_pred.csv")
cfg$paths$real_valid <- file.path("outputs", "valid_real.csv")
cfg$paths$pred_valid <- file.path("outputs", "valid_pred.csv")
cfg$paths$metrics_cal <- file.path("outputs", "metrics", "calib_metrics.csv")
cfg$paths$metrics_val <- file.path("outputs", "metrics", "valid_metrics.csv")
cfg$paths$run_info    <- file.path("outputs", "run_info.csv")
cfg$paths$errors_csv  <- file.path("outputs", "errors.csv")
cfg$paths$pattern_indices  <- file.path("outputs", "pattern_indices.csv")
# write config file
out_path <- file.path(settings_dir, "config_model.ini")
writeLines(render_ini_text(cfg), out_path, useBytes = TRUE)
# copy binary - one binary for all
# file.copy(file.path(normalize_dir(root_dir), "bin", basename(bin_path)), case_dir, overwrite = TRUE)
file.copy(bin_path, case_dir, overwrite = TRUE)
invisible(out_path)
}
n_cores <- parallel::detectCores() - 1  # use one less than total n cores
cl <- makeCluster(n_cores)
registerDoParallel(cl)
cat("Using", n_cores, "cores for parallel config generation\n")
foreach(i = 1:nrow(param_grid), .packages = c("data.table")) %dopar% {
row <- param_grid[i]
generate_config_case(
root_dir = "JKMNet_run",  # MJ local + MetaVO
id = row$id,
epoch = row$epochs,
data_source = data_path, # set path to data file or create folder data with data files in root dir
#bin_path = "/storage/projects-du-praha/czu-fzp-hcv/software/JKMNet/bin/JKMNet",  # MetaVO
bin_path = "bin/JKMNet",  # MJ local
trainer = row$trainer,
activation = row$activation,
transform = row$transform,
transform_alpha = row$alpha,
architecture = row$architecture,
inputs = row$inputs,
outputs = row$outputs,
copy_data_into_inputs = FALSE,  # data already in shared location,
remove_na_before_calib = TRUE
)
}
stopCluster(cl)
cat("✅ All configurations generated successfully\n")
# select IDs - only IDs with more then 100 data points
# minimum number of records required
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
min_n <- 100
max_ids <- 20 # total valid approx. 285
# count number of records for each ID
id_counts <- data_file[, .N, by = ID]
# filter only IDs with at least min_n rows
valid_ids <- id_counts[N >= min_n, ID]
# randomly select one or more IDs from the filtered list
ids <- sample(valid_ids, size = max_ids, replace = FALSE)  # randomly select ids
print(ids)
